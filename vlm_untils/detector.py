import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import warnings
import re
import json

warnings.filterwarnings("ignore")

class SemanticBrain:
    def __init__(self, model_path="Qwen/Qwen2-VL-7B-Instruct", device="cuda"):
        print(f"[Brain] Loading VLM: {model_path} (High Sensitivity Mode)...")
        try:
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                attn_implementation="sdpa",
                device_map="auto",
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            self.processor = AutoProcessor.from_pretrained(model_path, min_pixels=256*28*28, max_pixels=512*28*28)
            print("[Brain] âœ… Vision Online")
        except Exception as e:
            print(f"[Brain] âŒ Load Failed: {e}")
            raise e

    def detect_object(self, image_input, target_text):
        """
        å…¨çŸ¥æ„ŸçŸ¥æ¨¡å¼ï¼š
        1. ä¼˜å…ˆå¯»æ‰¾ target_textã€‚
        2. å¦‚æœæ²¡æœ‰æ¡†ä½†è¯­ä¹‰å­˜åœ¨ï¼Œè¿”å›ä¸­å¿ƒæ¡† (Soft Lock)ã€‚
        3. [æœªæ¥æ‰©å±•] è¿”å›æ‰€æœ‰å¯è§ç‰©ä½“çš„åˆ—è¡¨ï¼Œç”¨äºå»ºç«‹å›æº¯åœ°å›¾ã€‚
        """
        if not isinstance(image_input, Image.Image):
            image = Image.fromarray(image_input)
        else:
            image = image_input

        # ğŸ”´ [Prompt å‡çº§] å¼ºåˆ¶é«˜çµæ•åº¦ï¼Œå¹¶è¦æ±‚æè¿°
        # æˆ‘ä»¬é—®ä¸¤ä¸ªé—®é¢˜ï¼š1. æœ‰æ²¡æœ‰ï¼Ÿ 2. åœ¨å“ªé‡Œï¼Ÿ
        prompt_text = (
            f"Look carefully. Is there a '{target_text}' in this image? "
            f"Even if it is small, far away, or partial, say 'YES'. "
            f"If YES, provide the bounding box [ymin, xmin, ymax, xmax]. "
            f"Also describe what else you see briefly."
        )
        
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt_text},
            ],
        }]

        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt").to("cuda")

        with torch.no_grad():
            # å¢åŠ  token é•¿åº¦ï¼Œå…è®¸æ¨¡å‹å¤šâ€œæ€è€ƒâ€ä¸€ä¼šå„¿
            generated_ids = self.model.generate(**inputs, max_new_tokens=128)
        
        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
        output_text = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        
        # ğŸ”´ [è°ƒè¯•] æ‰“å° VLM çš„å¿ƒå£°
        if len(output_text.strip()) > 0:
            print(f"   ğŸ‘ï¸ [VLM Thought] {output_text[:100].replace(chr(10), ' ')}...", flush=True)

        # --- è§£æé€»è¾‘ (ä¸‰å±‚æ¼æ–—) ---

        # 1. å°è¯•æå–æ ‡å‡†åæ ‡æ¡†
        numbers = re.findall(r'\d+(?:\.\d+)?', output_text)
        nums = [float(x) for x in numbers]
        
        if len(nums) >= 4:
            # å–æœ€å4ä¸ªæ•°å­—ä½œä¸ºåæ ‡ (é˜²æ­¢å‰é¢æœ‰æ—¥æœŸç­‰æ•°å­—å¹²æ‰°)
            coords = nums[-4:]
            final_coords = [val / 1000.0 if val > 1.0 else val for val in coords]
            y1, x1, y2, x2 = final_coords
            ymin, ymax = sorted([y1, y2])
            xmin, xmax = sorted([x1, x2])
            
            # åªæœ‰å½“æ¡†å¤§åˆ°ä¸€å®šç¨‹åº¦æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆæ¡† (é˜²æ­¢å™ªç‚¹)
            if (ymax - ymin) * (xmax - xmin) > 0.001:
                return [xmin, ymin, xmax, ymax], "HARD_LOCK"

        # 2. [è½¯é”å®š - Soft Lock] 
        # å¦‚æœæ²¡æœ‰åæ ‡ï¼Œä½†æ¨¡å‹è¯´äº† "YES" æˆ–è€… æåˆ°äº†ç›®æ ‡åå­—ï¼Œè¯´æ˜å®ƒçœ‹è§äº†ï¼
        # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸èƒ½æ”¾è¿‡ï¼Œè¿”å›ä¸€ä¸ªâ€œä¸­å¿ƒè§†é‡æ¡†â€ï¼Œéª—æœºå™¨äººå¾€ä¸­é—´èµ°
        target_keywords = target_text.lower().split()
        response_lower = output_text.lower()
        
        # åˆ¤å®šï¼šæ˜¯å¦åŒ…å« "yes" ä¸”åŒ…å«ç›®æ ‡ç‰©ä½“å
        is_positive = "yes" in response_lower or any(k in response_lower for k in target_keywords if len(k)>2)
        
        if is_positive:
            print(f"   âš ï¸ [Soft Lock] VLM saw '{target_text}' but gave no coords. Moving forward to check!", flush=True)
            # è¿”å›å±å¹•ä¸­å¿ƒçš„ä¸€ä¸ªè™šæ„æ¡† [0.4, 0.4, 0.6, 0.6]
            return [0.4, 0.4, 0.6, 0.6], "SOFT_LOCK"

        return None, "NONE"

    def parse_movement_command(self, text_command):
        # ä¿æŒä¸å˜
        system_prompt = "Output JSON: {\"mode\": \"adjust\"|\"stop\", \"delta_pose\": {\"x\":0.0, \"y\":0.0, \"z\":0.0, \"yaw\":0.0, \"pitch\":0.0}}"
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": text_command}]
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.processor(text=[text], return_tensors="pt", padding=True).to("cuda")
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, max_new_tokens=128)
        output_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        try:
            start = output_text.find('{')
            end = output_text.rfind('}') + 1
            if start != -1 and end != -1:
                return json.loads(output_text[start:end].replace("'", '"'))
        except: pass
        return None

